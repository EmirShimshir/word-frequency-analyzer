package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
package main

import (
	"flag"
	"fmt"
	"os"
	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/processor"
	"word-frequency-analyzer/internal/provider"
	"word-frequency-analyzer/internal/splitter"
)

const (
	defaultChunkSize = 50 // 1MB
)

func main() {
	// Парсим аргументы командной строки
	dirPath := flag.String("dir", "", "Путь к директории с текстовыми файлами")
	minWordLen := flag.Int("minlen", 5, "Минимальная длина слова")
	topCount := flag.Int("top", 10, "Количество топ-слов для вывода")
	flag.Parse()

	if *dirPath == "" {
		fmt.Println("Необходимо указать директорию с файлами (--dir)")
		os.Exit(1)
	}

	// Создаем все компоненты
	fileProvider := provider.NewDiskFileReader()
	chunkSplitter := splitter.NewDefaultChunkReader()
	wordExtractor := extractor.NewRegexWordExtractor()
	wordCounter := counter.NewDefaultWordCounter()

	// Создаем процессор
	processor := processor.NewConcurrentProcessor(
		chunkSplitter,
		wordExtractor,
		wordCounter,
		defaultChunkSize,
	)

	// Получаем список файлов
	files, err := fileProvider.ListTextFiles(*dirPath)
	if err != nil {
		fmt.Printf("Ошибка при чтении директории: %v\n", err)
		os.Exit(1)
	}

	if len(files) == 0 {
		fmt.Println("В указанной директории не найдено текстовых файлов (.txt)")
		os.Exit(0)
	}

	fmt.Printf("Найдено %d текстовых файлов\n", len(files))

	// Запускаем обработку
	counts, err := processor.ProcessAll(files, *minWordLen)
	if err != nil {
		fmt.Printf("Ошибка при обработке файлов: %v\n", err)
		os.Exit(1)
	}

	// Получаем топ-N слов
	topWords := processor.GetTopWords(counts, *topCount)

	// Выводим результат
	fmt.Printf("\nТоп-%d слов с минимальной длиной %d:\n", *topCount, *minWordLen)
	for i, word := range topWords {
		fmt.Printf("%d. %s: %d\n", i+1, word.Word, word.Count)
	}

	//// Записываем результат в файл, если указан путь
	//if *outputFile != "" {
	//	err := processor.SaveToFile(topWords, *outputFile)
	//	if err != nil {
	//		fmt.Printf("Ошибка при сохранении результатов в файл: %v\n", err)
	//	} else {
	//		fmt.Printf("\nРезультаты сохранены в файл: %s\n", *outputFile)
	//	}
	//}
}

//// SaveToFile сохраняет результаты анализа в текстовый файл
//func SaveToFile(words []WordCount, filename string) error {
//	file, err := os.Create(filename)
//	if err != nil {
//		return err
//	}
//	defer file.Close()
//
//	writer := bufio.NewWriter(file)
//	for i, word := range words {
//		_, err := fmt.Fprintf(writer, "%d. %s: %d\n", i+1, word.Word, word.Count)
//		if err != nil {
//			return err
//		}
//	}
//
//	return writer.Flush()
//}

package counter

// WordCounter подсчитывает частоту слов
type WordCounter interface {
	Count(words []string) map[string]int
	MergeCounts(counts ...map[string]int) map[string]int
}

// DefaultWordCounter стандартная реализация подсчета слов
type DefaultWordCounter struct{}

// NewDefaultWordCounter создает новый счетчик слов
func NewDefaultWordCounter() *DefaultWordCounter {
	return &DefaultWordCounter{}
}

// Count подсчитывает частоту слов
func (c *DefaultWordCounter) Count(words []string) map[string]int {
	counts := make(map[string]int)
	for _, word := range words {
		counts[word]++
	}
	return counts
}

// MergeCounts объединяет несколько карт подсчета
func (c *DefaultWordCounter) MergeCounts(counts ...map[string]int) map[string]int {
	result := make(map[string]int)

	for _, count := range counts {
		for word, freq := range count {
			result[word] += freq
		}
	}

	return result
}
package extractor

import (
	"regexp"
)

// WordExtractor извлекает слова из текста
type WordExtractor interface {
	ExtractWords(text []byte, minLength int) []string
}

// RegexWordExtractor извлекает слова с помощью регулярных выражений
type RegexWordExtractor struct {
	// Регулярное выражение будет компилироваться для конкретной минимальной длины
	regexCache map[int]*regexp.Regexp
}

// NewRegexWordExtractor создает новый экстрактор слов
func NewRegexWordExtractor() *RegexWordExtractor {
	return &RegexWordExtractor{
		regexCache: make(map[int]*regexp.Regexp),
	}
}

// getOrCreateRegex возвращает скомпилированное регулярное выражение для указанной мин. длины
func (e *RegexWordExtractor) getOrCreateRegex(minLength int) *regexp.Regexp {
	if re, ok := e.regexCache[minLength]; ok {
		return re
	}

	pattern := "(?i)[a-zа-я]{" + string(rune('0'+minLength)) + ",}"
	re := regexp.MustCompile(pattern)
	e.regexCache[minLength] = re
	return re
}

// ExtractWords извлекает слова из текста с минимальной длиной
func (e *RegexWordExtractor) ExtractWords(text []byte, minLength int) []string {
	re := e.getOrCreateRegex(minLength)
	return re.FindAllString(string(text), -1)
}

package processor

import (
	"sort"
	"sync"

	"word-frequency-analyzer/internal/counter"
	"word-frequency-analyzer/internal/extractor"
	"word-frequency-analyzer/internal/splitter"
	"word-frequency-analyzer/pkg/models"
)

// Processor обрабатывает файлы параллельно
type Processor interface {
	ProcessAll(files []string, minWordLen int) (map[string]int, error)
	GetTopWords(counts map[string]int, topCount int) []models.WordCount
}

// ConcurrentProcessor реализует параллельную обработку файлов
type ConcurrentProcessor struct {
	splitter  splitter.ChunkReader
	extractor extractor.WordExtractor
	counter   counter.WordCounter
	chunkSize int
}

// NewConcurrentProcessor создает новый процессор с зависимостями
func NewConcurrentProcessor(
	splitter splitter.ChunkReader,
	extractor extractor.WordExtractor,
	counter counter.WordCounter,
	chunkSize int,
) *ConcurrentProcessor {
	return &ConcurrentProcessor{
		splitter:  splitter,
		extractor: extractor,
		counter:   counter,
		chunkSize: chunkSize,
	}
}

// ProcessAll параллельно обрабатывает все файлы
func (p *ConcurrentProcessor) ProcessAll(files []string, minWordLen int) (map[string]int, error) {
	var wg sync.WaitGroup
	resultCh := make(chan map[string]int, len(files))

	for _, file := range files {
		wg.Add(1)
		go func(filePath string) {
			defer wg.Done()

			// Обработка одного файла
			counts, err := p.processFile(filePath, minWordLen)
			if err != nil {
				// В реальном приложении здесь нужно обработать ошибку
				return
			}

			resultCh <- counts
		}(file)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// Собираем результаты из всех файлов
	var allCounts []map[string]int
	for counts := range resultCh {
		allCounts = append(allCounts, counts)
	}

	// Объединяем все подсчеты
	return p.counter.MergeCounts(allCounts...), nil
}

// processFile обрабатывает один файл параллельно по чанкам
func (p *ConcurrentProcessor) processFile(filePath string, minWordLen int) (map[string]int, error) {
	chunks, err := p.splitter.SplitFile(filePath, p.chunkSize)
	if err != nil {
		return nil, err
	}

	var wg sync.WaitGroup
	chunkResultCh := make(chan map[string]int, len(chunks))

	for _, chunk := range chunks {
		wg.Add(1)
		go func(data []byte) {
			defer wg.Done()

			// Извлекаем слова из чанка
			words := p.extractor.ExtractWords(data, minWordLen)

			// Подсчитываем частоту слов
			counts := p.counter.Count(words)

			chunkResultCh <- counts
		}(chunk)
	}

	// Закрываем канал после завершения всех горутин
	go func() {
		wg.Wait()
		close(chunkResultCh)
	}()

	// Собираем результаты из всех чанков
	var chunkCounts []map[string]int
	for counts := range chunkResultCh {
		chunkCounts = append(chunkCounts, counts)
	}

	// Объединяем подсчеты из всех чанков
	return p.counter.MergeCounts(chunkCounts...), nil
}

// GetTopWords возвращает топ-N слов из карты частот
func (p *ConcurrentProcessor) GetTopWords(counts map[string]int, topCount int) []models.WordCount {
	result := make([]models.WordCount, 0, len(counts))

	// Преобразуем map в slice для сортировки
	for word, count := range counts {
		result = append(result, models.WordCount{
			Word:  word,
			Count: count,
		})
	}

	// Сортируем по убыванию частоты
	sort.Slice(result, func(i, j int) bool {
		return result[i].Count > result[j].Count
	})

	// Ограничиваем количество результатов
	if len(result) > topCount {
		result = result[:topCount]
	}

	return result
}

package provider

import (
	"os"
	"path/filepath"
	"strings"
)

// FileReader отвечает за получение списка текстовых файлов
type FileReader interface {
	ListTextFiles(path string) ([]string, error)
}

// DiskFileReader реализует FileReader для работы с файловой системой
type DiskFileReader struct{}

// NewDiskFileReader создает новый провайдер файлов
func NewDiskFileReader() *DiskFileReader {
	return &DiskFileReader{}
}

// ListTextFiles возвращает все .txt файлы в указанной директории
func (p *DiskFileReader) ListTextFiles(path string) ([]string, error) {
	var textFiles []string

	err := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(filePath, ".txt") {
			textFiles = append(textFiles, filePath)
		}

		return nil
	})

	return textFiles, err
}

package splitter

import (
	"io/ioutil"
)

// ChunkReader делит файл на части для параллельной обработки
type ChunkReader interface {
	SplitFile(path string, chunkSize int) ([][]byte, error)
}

// DefaultChunkReader — быстрая реализация без перекрытия и без строк
type DefaultChunkReader struct{}

// NewDefaultChunkReader создаёт сплиттер
func NewDefaultChunkReader() *DefaultChunkReader {
	return &DefaultChunkReader{}
}

// isSpace проверяет, является ли байт пробелом/переводом строки и т.п.
func isSpace(b byte) bool {
	// Только ASCII-пробельные символы
	return b == ' ' || b == '\n' || b == '\t' || b == '\r'
}

// SplitFile делит данные, не разрывая слова и не переходя к строкам
func (s *DefaultChunkReader) SplitFile(path string, chunkSize int) ([][]byte, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var chunks [][]byte
	start := 0

	for start < len(data) {
		end := start + chunkSize
		if end >= len(data) {
			end = len(data)
			chunks = append(chunks, data[start:end])
			break
		}

		// Найдём последний пробел до end
		split := end
		for split > start && !isSpace(data[split]) {
			split--
		}

		// если пробела не нашли — жертвуем словом, идём до следующего пробела
		if split == start {
			split = end
			for split < len(data) && !isSpace(data[split]) {
				split++
			}
		}

		// trim правые пробелы
		for split > start && isSpace(data[split-1]) {
			split--
		}

		chunks = append(chunks, data[start:split])

		// сдвигаем старт: пропускаем пробелы
		start = split
		for start < len(data) && isSpace(data[start]) {
			start++
		}
	}

	return chunks, nil
}

package models

// WordCount представляет пару слово-частота
type WordCount struct {
	Word  string
	Count int
}

// Result содержит отсортированный список слов
type Result struct {
	Words []WordCount
}
